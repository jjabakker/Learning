---
title: "Statistics"
author: "Hans"
date: "9/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(tidyverse)
p_load(nycflights13)
p_load(ggplot2)
p_load(Deriv)
p_load(phaseR)
p_load(gridExtra)
theme_set(theme_minimal())

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}
```

# Unclear 
Take samples of an increasing size and repeat that 5 times   
The standard deviation decreases with the increasing set size

```{r}

(sd  = 1/12)

for (n in c(2, 10, 100, 1000)) {
  
  cat ("\n5 runs for n = ", n, "\n")
  for (i in 1:5) {
    s = runif(n)
    print(sd(s))
  }
}
```


# Standard Error of the mean

## Uniform distribution


```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(runif(nosim * n), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a uniform distrbution the variance is 1/12
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 1/12
sd = sqrt(var) 
t = sd/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```


## Normal distribution

```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(rnorm(nosim * n), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a normal distribution the variance is 1
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 1
sd = sqrt(var) 
t = sd/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```


## Poisson distribution

```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(rpois(nosim * n, 4), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a poisson(4) distribution the variance is 4
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 4
sd = sqrt(var) 
t = sd/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```



# Exploring Distributions

## Normal
```{r}
# Use the 'd' functions to get the density probability 
dnorm(0)
x <- seq(-4,4,0.1)
y <- dnorm(x)
ggplot() + 
  geom_line(aes(x = x, y = y)) + 
  labs(title = 'Density - dnorm')
```

```{r}

# Use the 'p' functions to get the cumulative probability 
pnorm(0, mean = 0, sd = 1)
pmin1  = pnorm(-1, mean = 0, sd = 1)
pplus1 = pnorm(+1, mean = 0, sd = 1)
cat ("The +/- sd interval is:", pplus1 - pmin1)
y = pnorm(x)
ggplot() + 
  geom_line(aes(x = x, y = y)) + 
  labs(title = 'Cumulative - pnorm')
```

```{r}

pmin2  = pnorm(-2, mean = 0, sd = 1)
pplus2 = pnorm(+2, mean = 0, sd = 1)
cat ("The +/- 2 sd interval is:", pplus2 - pmin2,"\n")

pmin196  = pnorm(-1.96, mean = 0, sd = 1)
pplus196 = pnorm(+1.96, mean = 0, sd = 1)
cat ("The +/- 1.96 sd interval is:", pplus196 - pmin196,"\n")

pmin3  = pnorm(-3, mean = 0, sd = 1)
pplus3 = pnorm(+3, mean = 0, sd = 1)
cat ("The +/- 3 sd interval is:", pplus3 - pmin3,"\n")
```

```{r}

# Use the 'q' functions for quartile
# This is the inverse of the probability function

x = seq(0, 1, 0.2) 
qbinom(x, size = 50, prob = .33)

q10 = qnorm(0.10, mean = 0, sd = 1)
q25 = qnorm(0.25, mean = 0, sd = 1)
q50 = qnorm(0.50, mean = 0, sd = 1)
q75 = qnorm(0.75, mean = 0, sd = 1)
q90 = qnorm(0.90, mean = 0, sd = 1)

pnorm(q10)
pnorm(q25)
pnorm(q50)
pnorm(q75)
pnorm(q75)

# (Or do the same thing quicker)
p = c(0.1, 0.25, 0.5, 0.75, 0.9)
p
q = qnorm(p, mean = 0, sd = 1)
q

# Use the 'r' functions for random samples
rnorm(10, mean = 5, sd = 2)
```

### Plots

```{r}
# Generate plots
x <- seq(-4,4,0.1)
d <- dnorm(x)
p <- pnorm(x)
r <- rnorm(x)


ggplot() + 
  geom_line(aes(x = x, y = d)) +
  geom_line(aes(x = x, y = p))


ggplot() + 
  geom_point(aes(x = x, y = r)) + 
  labs(title = 'Sample - rnorm')

x <- seq(0,1,0.001)
q <- qnorm(x)

ggplot() + 
  geom_line(aes(x = x, y = q)) + 
  labs(title = 'Quantile - qnorm')

```

## Binominal

```{r}

x <- 0:20
y <- dbinom(x, size = 7, prob = .5)
ggplot() + geom_bar(aes(x=x, y=y), stat = "identity", width=0.2)

# Use the 'p' functions to get the cumulative probability 
# What is the change to have x successes - or more - out of 8 tests with a chance 0.5 
pbinom(6, size = 8, prob = 0.5, lower.tail = F)

# Use the 'q' functions for quartile
# This is the inverse of the probability function

# How many successes do I need to have a chance of 50% that I achieve the test 
qbinom(0.5, size = 8, prob = 0.5)

# How many successes do I need to have a chance a that I achieve of 75% test
qbinom(0.75, size = 8, prob = 0.5)

# Use the 'r' functions for random samples
rbinom(25, size = 50, prob = 0.5)
```


# T-Test

## Paired T-test

```{r}
tdata = read_csv("T-testData.csv")

# Single test on the difference 
ts = t.test(tdata$Difference)
ts

# Paired test
tp = t.test(x=tdata$Before, y=tdata$After, paired = TRUE)
tp

# Fully identical results!
```


## T-distribution plots
```{r}
# Generate a t-distribution with 200 points
y <- rt(200, df = 5)

# Plot the distribution against a normal distibution 
qqnorm(y); 
qqline(y, col = 'red')

# Or plot two sets against each other
z <- rt(300, df = 5)
qqplot(y,  z)

z <- rt(200, df = 3)
qqplot(y,  z)

n1 = rnorm(1000)
qqnorm(n1)

qqplot(n1,n1)

```

## T example

```{r}
# Ref: http://rcompanion.org/rcompanion/c_01.html

p_load(psych)
data <- tribble (
  ~Stream, ~Fish,
  "Mill_Creek_1", 76,
  "Mill_Creek_2", 102, 
  "North_Branch_Rock_Creek_1", 12,
  "North_Branch_Rock_Creek_2", 39, 
  "Rock_Creek_1", 55, 
  "Rock_Creek_2", 93, 
  "Rock_Creek_3", 98,
  "Rock_Creek_4", 53,
  "Turkey_Branch", 102)

describe(data$Fish)

mean(data$Fish)
median(data$Fish)
mode(data$Fish)
geometric.mean(data$Fish)     
harmonic.mean(data$Fish)     

var(data$Fish)
sd(data$Fish)
range(data$Fish)
IQR(data$Fish)  

(n    = length(data$Fish))
(var  = sum((data$Fish - mean(data$Fish))^2)/(length(data$Fish) - 1))
(sdev = sqrt(var))
(sem  = sdev / sqrt(n))
(std_err(data$Fish))

# Confidence interval via t.test
(tt = t.test(data$Fish))

# Confidence interval manually, use the T distribution!
q = qt(0.975, length(data$Fish) - 1)
(ci_low = mean(data$Fish) - sem*q)
(ci_hi  = mean(data$Fish) + sem*q)

summary(tt)
glimpse(tt)
```

```{r Normalised, manual using z-scores}

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}

# You can use 12 here, because that was a number in the H0 hypothesis 
sem         = std_err(sample)
sample_mean = mean(sample)
(zscore     = (sample_mean - 12) / std_err(sample))
(cl95 <- qnorm(0.975))


```


## Manual using t-values

```{r}

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}

# Generate a 16 sample set  with mean = 13.5 and sd = 2
sample <- rnorm(16, 13.5, 2)

# Manual
mu   <- mean(sample)
sd   <- sd(sample)
n    <- length(sample)
df   <- length(sample)-1
sem  <- sd / sqrt(n)
tval <- (mu - 12) / std_err(sample)
clim <- qt(.975, df = df) 
ci   <- mu + c(-clim, clim) * std_err(sample)
pval <- 2 * pt(-abs(tval), df = df) # -abs(tvalue) to get lower tail

cat("t = ", tval, ", df = ", df, ", p-value = ", pval, "\n")
cat("95 percent confidence interval:",ci[1], ci[2], "\n")
cat("mean of x", mu)

# Autmatic
t.test(sample, mu = 12)

```

# Quantile

```{r}
x = rnorm(10000000, 0, 1)
quantile(x, probs = 0.5)    # Should be approximately at 0

# This selects the small tail on the left that is associated with x = -2
quantile(x, probs = 0.025)

# This selects the tail on the left that is associated with x = -1
quantile(x, probs = 0.16)

# This selects the small tail on the left that is associated with x = -3
quantile(x, probs = 0.0015)
```


# Law of large numbers

```{r}
# What does cumsum do?
(x  <- 1:10)
(cs <- cumsum(x))
(y  <- cs/x)

# Now the example
# With increasing numbers the mean of the repeated normal draws creep to 0

n <- 10000
means <- cumsum(rnorm(n))/(1:n)

g <- ggplot(data.frame(x = 1:n, y = means), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0) + geom_line(size = 0.5)
g <- g + labs(x = "Number of obs", y = "Cumulative mean")
g
```

```{r}

# With increasing numbers the mean of the repeated coin flip creeps to 0.5
means <- cumsum(sample(0:1, n, replace = TRUE))/(1:n)
g <- ggplot(data.frame(x = 1:n, y = means), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0.5) + geom_line(size = 0.5)
g
```


# Central Limit Theory

You can generate a normal ditricutiin by throwinbg a dice 6 times.
Then you take the mean and normalise it by cubtracting the population mean (3.5) and by dividing by the population sd/sqrt(n).

From n = 6 onwards you get a good match

```{r}

x = seq(-4,4,0.1)
y = dnorm(x)

for_distribution = vector(mode = 'numeric',100)
for (i in 1:1000) {
  n = 1
  sample_dice <- sample(1:6, n, replace = TRUE)
  mean_sample <- mean(sample_dice)
  for_distribution[i] <- (mean_sample - 3.5)/(1.71/sqrt(n))
}
  

d <- density(for_distribution)
plot(d)

ggplot() + 
  geom_density(aes(x=for_distribution)) +
  geom_line(mapping = aes(x=x, y=y), colour = 'red')
```

```{r}
p_load(UsingR)
data(father.son)
t.test(father.son$sheight - father.son$fheight)
```

