---
title: "Statistics"
author: "Hans"
date: "9/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(tidyverse)
p_load(nycflights13)
p_load(ggplot2)
p_load(Deriv)
p_load(phaseR)
p_load(gridExtra)
theme_set(theme_minimal())

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}
```

# Standard Error of the mean

## Uniform distribution

Take samples of an increasing size and repeat that 5 times   
The standard deviation decreases with the increasing set size

```{r}

# sd  = 1/12

for (n in c(2, 10, 100, 1000)) {
  
  cat ("\n5 runs for n = ", n, "\n")
  for (i in 1:5) {
    print(sd(runif(n)))
  }
}
```

```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(runif(nosim * n), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a uniform distrbution the variance is 1/12
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 1/12
t = sqrt(var)/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```


## Normal distribution

```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(rnorm(nosim * n), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a normal distribution the variance is 1
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 1
t = sqrt(var)/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```


## Poisson distribution

```{r}
cat("\n")
nosim <- 1000
n <- 10

# Create a matrix of nosim by n 
dat = matrix(rpois(nosim * n, 4), nosim, n)

# Calculate the mean for every column
datm = apply(dat, 1, mean)

# Calculate the mean and sd of that column

sdev = sd(datm)
smean = mean(datm)

# For a poisson(4) distribution the variance is 4
# The SD is sqrt(Var) 
# The SEM is SD / sqrt(n)

var = 4
t = sqrt(var)/sqrt(n)
cat('The observed sem is', sdev, 'compared to the theoretical',t)
```



# Distributions

## Normal
```{r}

# Use the 'd' functions to get the density probability 
dnorm(0)
x <- seq(-4,4,0.1)
y <- dnorm(x)
ggplot() + geom_line(aes(x = x, y = y)) + labs(title = 'Density - dnorm')

# Use the 'p' functions to get the cumulative probability 
pnorm(0, mean = 0, sd = 1)
pmin1  = pnorm(-1, mean = 0, sd = 1)
pplus1 = pnorm(+1, mean = 0, sd = 1)
cat ("The +/- sd interval is:", pplus1 - pmin1)

y = pnorm(x)
ggplot() + geom_line(aes(x = x, y = y)) + labs(title = 'Cumulative - pnorm')

pmin2  = pnorm(-2, mean = 0, sd = 1)
pplus2 = pnorm(+2, mean = 0, sd = 1)
pplus2 - pmin2
cat ("The +/- 2sd interval is:", pplus2 - pmin2)

pmin196  = pnorm(-1.96, mean = 0, sd = 1)
pplus196 = pnorm(+1.96, mean = 0, sd = 1)
pplus196 - pmin196
cat ("The +/- 2sd interval is:", pplus196 - pmin196)

pmin3  = pnorm(-3, mean = 0, sd = 1)
pplus3 = pnorm(+3, mean = 0, sd = 1)
cat ("The +/- 3sd interval is:", pplus3 - pmin3)

# Use the 'q' functions for quartile
# This is the inverse of the probability function

qbinom(x, size = 50, prob = .33)

q10 = qnorm(0.10, mean = 0, sd = 1)
q25 = qnorm(0.25, mean = 0, sd = 1)
q50 = qnorm(0.50, mean = 0, sd = 1)
q75 = qnorm(0.75, mean = 0, sd = 1)
q90 = qnorm(0.90, mean = 0, sd = 1)

pnorm(q10)
pnorm(q25)
pnorm(q50)
pnorm(q75)
pnorm(q75)

# (Or do the same thing quicker)
p = c(0.1, 0.25, 0.5, 0.75, 0.9)
p
q = qnorm(p, mean = 0, sd = 1)
q

# Use the 'r' functions for random samples
rnorm(10, mean = 5, sd = 2)
```

```{r}
# Generate plots
x <- seq(-4,4,0.1)
d <- dnorm(x)
p <- pnorm(x)
r <- rnorm(x)


ggplot() + 
  geom_line(aes(x = x, y = d)) +
  geom_line(aes(x = x, y = p))


ggplot() + 
  geom_point(aes(x = x, y = r))+ labs(title = 'Sample - rnorm')

x <- seq(0,1,0.001)
q <- qnorm(x)

ggplot() + 
  geom_line(aes(x = x, y = q)) + labs(title = 'Quantile - qnorm')

```

# Binominal

```{r}

x <- 0:20
y <- dbinom(x, size = 7, prob = .5)
ggplot() + geom_bar(aes(x=x, y=y), stat = "identity", width=0.2)

# Use the 'p' functions to get the cumulative probability 
# What is the change to have x successes - or more - out of 8 tests with a chance 0.5 
pbinom(6, size = 8, prob = 0.5, lower.tail = F)

# Use the 'q' functions for quartile
# This is the inverse of the probability function

# How many successes do I need to have a chance of 50% that I achieve the test 
qbinom(0.5, size = 8, prob = 0.5)

# How many successes do I need to have a chance a that I achieve of 75% test
qbinom(0.75, size = 8, prob = 0.5)

# Use the 'r' functions for random samples
rbinom(25, size = 50, prob = 0.5)
```


```{r}
tdata = read_csv("T-testData.csv")

# Single test on the difference 
ts = t.test(tdata$Difference)
ts

# Paired test
tp = t.test(x=tdata$Before, y=tdata$After, paired = TRUE)
tp

# Fully identical results!
```


```{r}
# Generate a t distribution
y <- rt(200, df = 5)

# Plot the distribution against a normal distibution 
qqnorm(y); 
qqline(y, col = 'red')

# Or plot two sets against each other
z <- rt(300, df = 5)
qqplot(y,  z)

z <- rt(200, df = 3)
qqplot(y,  z)

n1 = rnorm(100000)
qqnorm(n1)

qqplot(n1,n1)

```

```{r}
# Ref: http://rcompanion.org/rcompanion/c_01.html

p_load(psych)
data <- tribble (
  ~Stream, ~Fish,
  "Mill_Creek_1", 76,
  "Mill_Creek_2", 102, 
  "North_Branch_Rock_Creek_1", 12,
  "North_Branch_Rock_Creek_2", 39, 
  "Rock_Creek_1", 55, 
  "Rock_Creek_2", 93, 
  "Rock_Creek_3", 98,
  "Rock_Creek_4", 53,
  "Turkey_Branch", 102)

describe(data$Fish)

mean(data$Fish)
median(data$Fish)
mode(data$Fish)
geometric.mean(data$Fish)     
harmonic.mean(data$Fish)     

var(data$Fish)
sd(data$Fish)
range(data$Fish)
IQR(data$Fish)  

(var  =  sum((data$Fish - mean(data$Fish))^2)/(length(data$Fish) - 1))
(sdev =  sqrt(var))
(sem = sdev / sqrt(length(data$Fish)))
(std_err(data$Fish))

# Confidence interval via t.test
(tt = t.test(data$Fish))

# Confidence interval manually, use the T distribution!
q = qt(0.975, length(data$Fish) - 1)
(ci_low = mean(data$Fish) - sem*q)
(ci_hi  = mean(data$Fish) + sem*q)

summary(tt)
glimpse(tt)
```

```{r Normalised, manual using z-scores}

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}

# You can use 12 here, because that was a number in the H0 hypothesis 
sem = std_err(sample)
sample_mean= mean(sample)
(zscore = (sample_mean - 12) / std_err(sample))
(cl95 <- qnorm(0.975))

#p <- dnorm_gg()
#p <- p + geom_vline(aes(xintercept = x), data = tibble(x = c(-cl95, cl95)), color = "red")
#p <- p + geom_vline(xintercept = zscore, color = "blue")
#p
```


# Manual using t-values

```{r}

std_err <- function(sample) {
  sd(sample) / sqrt(length(sample)) 
}

# Generate a 16 sample set 
sample <- rnorm(16, 13.5, 2)

# Manual
mu   <- mean(sample)
sd   <- sd(sample)
df   <- length(sample)-1
sem  <- sd / sqrt(length(sample))
tval <- (mu - 12) / std_err(sample)
clim <- qt(.975, df = df) 
ci   <- mu + c(-clim, clim) * std_err(sample)
pval <- 2 * pt(-abs(tval), df = df) # -abs(tvalue) to get lower tail

cat("t = ", tval, ", df = ", df, ", p-value = ", pval)
cat("95 percent confidence interval:")
cat(ci[1], ci[2])
cat("mean of x", mu)

# Autmatic
t.test(sample, mu = 12)

```

# Quantile

```{r}
x = rnorm(10000000, 0, 1)
quantile(x, probs = 0.5)    # Should be approximately at 0

# This selects the small tail on the left that is associated with x = -2
quantile(x, probs = 0.025)

# This selects the tail on the left that is associated with x = -1
quantile(x, probs = 0.16)

# This selects the small tail on the left that is associated with x = -3
quantile(x, probs = 0.0015)
```

